{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_doctors_review_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1dk9OkUsnZc"
      },
      "source": [
        "# Generating german doctor reviews with a GPT-2 model\n",
        "## Fine tuning of a pretrained **Hugging Face** transfomer decoder\n",
        "In this notebook we will be using a GPT-2 mdoel that was fine-tuned to synthesize doctor reviews mimiking actual patients' text comments.\n",
        "\n",
        "A detailed description of the **German language reviews of doctors by patients 2019** dataset can be found [here](https://data.world/mc51/german-language-reviews-of-doctors-by-patients)\n",
        "\n",
        "\n",
        "For this exercise, we will use the [**Hugging Face**](https://huggingface.co/) implementation of transformers for Tensorflow 2.0. Transformers provides a general architecture implementation for several state of the art models in the natural language domain.\n",
        "\n",
        "NOTE: This notebook and its implementation is heavily influenced by the data-drive [blog post](https://data-dive.com/finetune-german-gpt2-on-tpu-transformers-tensorflow-for-text-generation-of-reviews)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkqTxWLHouAC"
      },
      "source": [
        "!pip install -U transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_fTsD6Lp49I"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
        "\n",
        "pd.options.display.max_colwidth = 600\n",
        "pd.options.display.max_rows = 400"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoNm6fYGtFv6"
      },
      "source": [
        "## Setting up the decoder model\n",
        "HuggingFace's transfomer library allows for conviniently loading  pre-configured text tokenizers and pre-trained models from local resources.\n",
        "\n",
        "Here we will be using a tokenizer and a GPT-2 model that was pre-trained on the doctor review dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLAmgz3Rr6_W"
      },
      "source": [
        "!wget -O gpt2_doctorreview_finetuned.zip https://github.com/AdvancedNLP/decoder/raw/main/gpt2_doctorreview_finetuned.zip\n",
        "!unzip gpt2_doctorreview_finetuned.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZWMXIHmsDzg"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt2_doctorreview_finetuned/tokenizer')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2_doctorreview_finetuned/model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp158R8YyP5V"
      },
      "source": [
        "## Generating doctor reviews\n",
        "The model has been conditioned to be able to control if positive or negative reviews should be generated. \n",
        "\n",
        "As an auto-regressive model the sequence is generated by building up from the passed input sequences. We can use this to control the polarity of the review by passing either the token for positive or for negative reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0TUof0E3NLP"
      },
      "source": [
        "POS_TOKEN = \"<|review_pos|>\"\n",
        "NEG_TOKEN = \"<|review_neg|>\""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-6EYxJDy6pq"
      },
      "source": [
        "#### High level pipeline\n",
        "The easiest way to to use the model is to use HuggingFaces transformer `pipeline` implementation to encapsulate the previously loaded `model` and `tokenizer`.\n",
        "\n",
        "The documentation for the [**pipeline**](https://huggingface.co/transformers/main_classes/pipelines.html) abstraction describes how to do the setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xaNvHGq5u9F"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oa024P7zzVy"
      },
      "source": [
        "##########################\n",
        "## YOUR CODE HERE START ##\n",
        "##########################\n",
        "# build a transformer-pipeline \n",
        "# to generate text using the \n",
        "# previously loaded model and tokenizer\n",
        "\n",
        "review_generator = pipeline(\n",
        "  \"text-generation\",\n",
        "  model=model,\n",
        "  tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE END   ##\n",
        "##########################"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhj8_NG_ms2T"
      },
      "source": [
        "pos_generated_reviews = review_generator(POS_TOKEN, max_length=150, num_return_sequences=6)\n",
        "pd.DataFrame(pos_generated_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxN9_D_Nm3jT"
      },
      "source": [
        "neg_generated_reviews = review_generator(NEG_TOKEN, max_length=150, num_return_sequences=6)\n",
        "pd.DataFrame(neg_generated_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnqHWi9U0fnQ"
      },
      "source": [
        "### Low level control over text generation\n",
        "So far so good but sometimes you will need more control over how the text is being generated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s8ock9e3x7S"
      },
      "source": [
        "#### Greedy search\n",
        "The following code can be used to generate text using a greedy search algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw7I4kKh0OGJ"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(POS_TOKEN, return_tensors='tf')\n",
        "\n",
        "# generate text until the output length\n",
        "# (which includes the context length) reaches 50 \n",
        "greedy_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50,\n",
        "    num_return_sequences=3,\n",
        "    )\n",
        "\n",
        "genrated_reviews = [{'generated_text': tokenizer.decode(output, skip_special_tokens=True)}\n",
        "                    for output in greedy_outputs]\n",
        "pd.DataFrame(genrated_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdKzZbwZ1_fP"
      },
      "source": [
        "#### Beam search \n",
        "Beam search can be considered as an alternative. At each step of generating a token, a set of top probability tokens are kept as part of the beam instead of just the highest-probability token. The sequence with the highest overall probability is returned at the end of the generation.\n",
        "\n",
        "What do the parameters `no_repeat_ngram_size` and `temperature` control?\n",
        "\n",
        "Generating text using beam search is done like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gARjZeRd02rK"
      },
      "source": [
        "beam_outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=7,\n",
        "    no_repeat_ngram_size=3,\n",
        "    num_return_sequences=3,\n",
        "    early_stopping=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "genrated_reviews = [{'generated_text': tokenizer.decode(output, skip_special_tokens=True)}\n",
        "                    for output in beam_outputs]\n",
        "pd.DataFrame(genrated_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrsF91iV3Bbu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}