{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT2_doctors_review_eval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1dk9OkUsnZc"
      },
      "source": [
        "# Generating german doctor reviews with a GPT-2 model\n",
        "## Fine tuning of a pretrained **Hugging Face** transfomer decoder\n",
        "In this notebook we will be using a GPT-2 mdoel that was fine-tuned on the German doctors review dataset, to synthesize doctor reviews mimiking actual patients' text comments.\n",
        "\n",
        "You will implement and use different ways to invoke the model and generate new text, token by token. Along the way you will load the custom pre-trained GPT-2 model, use Tensorflow to implement a simple greedy search approach and finally also explore more advanced approaches.\n",
        "\n",
        "\n",
        "A detailed description of the **German language reviews of doctors by patients 2019** dataset can be found [here](https://data.world/mc51/german-language-reviews-of-doctors-by-patients)\n",
        "\n",
        "\n",
        "For this exercise, we will use the [**Hugging Face**](https://huggingface.co/) implementation of transformers for Tensorflow 2.0. \n",
        "\n",
        "NOTE: This notebook and its implementation is heavily influenced by the [data-drive](https://data-dive.com/) *Natural Language Processing of German texts* blog post"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkqTxWLHouAC"
      },
      "source": [
        "!pip install -U transformers==4.32.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_fTsD6Lp49I"
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import AutoTokenizer, TFGPT2LMHeadModel\n",
        "\n",
        "pd.options.display.max_colwidth = 600\n",
        "pd.options.display.max_rows = 400"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoNm6fYGtFv6"
      },
      "source": [
        "## Setting up the decoder model\n",
        "HuggingFace's transfomer library allows for conviniently loading  pre-configured text tokenizers and pre-trained models from local resources.\n",
        "\n",
        "Here we will be using a tokenizer and a GPT-2 model that was pre-trained on the doctor review dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLAmgz3Rr6_W"
      },
      "source": [
        "!rm -r gpt2_doctorreview_finetuned* __MACOSX\n",
        "!gdown https://drive.google.com/uc?id=13wbf5bsLmvRFD-AgbmruWo6bdiyjkwd9 -O gpt2_doctorreview_finetuned.zip\n",
        "!unzip gpt2_doctorreview_finetuned.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NohLX8gHGcO8"
      },
      "source": [
        "### Transformer artifacts\n",
        "What has been downloaded? \n",
        "Inspect the file system and get an intuition how HuggingFace transformers store the neccessary information to be re-initialized.\n",
        "What do the different files represent? Why does the vocabulary look the way it looks?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZWMXIHmsDzg"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gpt2_doctorreview_finetuned/tokenizer')\n",
        "model = TFGPT2LMHeadModel.from_pretrained('gpt2_doctorreview_finetuned/model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp158R8YyP5V"
      },
      "source": [
        "## Generating doctor reviews\n",
        "The model has been conditioned to be able to control if positive or negative reviews should be generated. \n",
        "\n",
        "As an auto-regressive model the sequence is generated by building up from the passed input sequences. We can use this to control the polarity of the review by passing either the token for positive or for negative reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0TUof0E3NLP"
      },
      "source": [
        "POS_TOKEN = \"<|review_pos|>\"\n",
        "NEG_TOKEN = \"<|review_neg|>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bH8NmbrHYj8"
      },
      "source": [
        "### Simple greedy search\n",
        "Let's implement our own greedy-search-based text generator. Generation happens in a loop where one token is generated at a time. Token with highest probability is select in each iteration.\n",
        "\n",
        "**HINT**:\n",
        "- Useful functions for this exercise:\n",
        " - [`tf.math.argmax`](https://www.tensorflow.org/api_docs/python/tf/math/argmax)\n",
        " - [`tf.cast`](https://www.tensorflow.org/api_docs/python/tf/cast)\n",
        " - [`tf.expand_dims`](https://www.tensorflow.org/api_docs/python/tf/expand_dims)\n",
        " - [`tf.concat`](https://www.tensorflow.org/api_docs/python/tf/concat)\n",
        "- Tensorflow tensors can be sliced using the same synthax as with Numpy Array\n",
        " - Use slicing to retrieve specific tensor parts\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_0JbcK0Hcqj"
      },
      "source": [
        "def generate_greedy(inputs:str, max_length=15):\n",
        "    #Generates a text sequence from an input str using a greedy search\n",
        "\n",
        "    input_ids = tokenizer.encode(inputs, return_tensors='tf')\n",
        "\n",
        "    ##########################\n",
        "    ## YOUR CODE HERE START ##\n",
        "    ##########################\n",
        "\n",
        "\n",
        "    # loop until the max_length is reached\n",
        "    for ...\n",
        "        \n",
        "        logits = model.predict(input_ids).logits\n",
        "\n",
        "        # retrieve the predicted logits for the *last* token\n",
        "        # Dimensions are [batch_size, input_tokens, vocab_size]\n",
        "        next_token_logits = ....\n",
        "\n",
        "        # Select the token with the highest probability\n",
        "        next_token = ....\n",
        "\n",
        "        # convert it to tf.int32\n",
        "        next_token = tf.cast(next_token, tf.int32)\n",
        "\n",
        "        # We have to expand the dimension of the next token to match \n",
        "        # the shape of input_ids\n",
        "        next_token = tf.expand_dims(next_token, axis=-1)\n",
        "\n",
        "        # Now we need to add the next_token to the previous ones.\n",
        "        # Concat input_ids with the next_token\n",
        "        input_ids = ....\n",
        "\n",
        "\n",
        "    output_ids = input_ids.numpy().squeeze()\n",
        "\n",
        "    ### Use your tokenizer to decode the output ids into text\n",
        "    decoded = ....\n",
        "\n",
        "    ##########################\n",
        "    ## YOUR CODE HERE END ##\n",
        "    ##########################\n",
        "    return decoded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you invoke you greedy search implementation you will get a generated text conditioned on the beginning of the input-text.\n",
        "\n",
        "Using `POS_TOKEN` will increase the likelihood to generate a positive text. While using `NEG_TOKEN` will result in a rather negative text."
      ],
      "metadata": {
        "id": "hgdRvSVAoS2N"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJQ44yf1H_GH"
      },
      "source": [
        "generate_greedy(POS_TOKEN + ' Ich', max_length=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6J10wZXfxDf"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnqHWi9U0fnQ"
      },
      "source": [
        "### More advanced text generation\n",
        "So far so good. Now we understand how text can be generated.\n",
        "\n",
        "However we ignore when our model predicts EOS (end-of-sentence). What would be neccessary to incoorporate this in our function?\n",
        "\n",
        "What if we would want to generate multiple different review comments?\n",
        "Did you generate long reviews? Have you started to see repetitions in the generated output? Why is that?\n",
        "\n",
        "Luckily the Hugging Face implementation offers various ways for us to generate higher quality reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s8ock9e3x7S"
      },
      "source": [
        "#### Greedy search\n",
        "The following code can be used to generate text using a greedy search algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw7I4kKh0OGJ"
      },
      "source": [
        "# encode context the generation is conditioned on\n",
        "input_ids = tokenizer.encode(POS_TOKEN, return_tensors='tf')\n",
        "\n",
        "# generate text until the output length\n",
        "# (which includes the context length) reaches 50 \n",
        "greedy_outputs = model.generate(\n",
        "    input_ids, \n",
        "    max_length=50,\n",
        "    num_return_sequences=3,\n",
        "    )\n",
        "\n",
        "genrated_reviews = [{'generated_text': tokenizer.decode(output, skip_special_tokens=True)}\n",
        "                    for output in greedy_outputs]\n",
        "pd.DataFrame(genrated_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdKzZbwZ1_fP"
      },
      "source": [
        "#### Beam search \n",
        "Beam search can be considered as an alternative. At each step of generating a token, a set of top probability tokens are kept as part of the beam instead of just the highest-probability token. The sequence with the highest overall probability is returned at the end of the generation.\n",
        "\n",
        "What do the parameters `no_repeat_ngram_size` and `temperature` control?\n",
        "\n",
        "Generating text using beam search is done like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gARjZeRd02rK"
      },
      "source": [
        "beam_outputs = model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=7,\n",
        "    no_repeat_ngram_size=3,\n",
        "    num_return_sequences=3,\n",
        "    early_stopping=True,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "genrated_reviews = [{'generated_text': tokenizer.decode(output, skip_special_tokens=True)}\n",
        "                    for output in beam_outputs]\n",
        "pd.DataFrame(genrated_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-6EYxJDy6pq"
      },
      "source": [
        "#### High level pipeline\n",
        "The easiest way to to use the model is to use HuggingFaces transformer `pipeline` implementation to encapsulate the previously loaded `model` and `tokenizer`.\n",
        "\n",
        "The documentation for the [**pipeline**](https://huggingface.co/transformers/main_classes/pipelines.html) abstraction describes how to do the setup.\n",
        "\n",
        "While being able to generate reviews with very high fiddelity, it's also the slowest approach. Can you find out why?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xaNvHGq5u9F"
      },
      "source": [
        "from transformers import pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Oa024P7zzVy"
      },
      "source": [
        "##########################\n",
        "## YOUR CODE HERE START ##\n",
        "##########################\n",
        "# build a transformer-pipeline \n",
        "# to generate text using the \n",
        "# previously loaded model and tokenizer\n",
        "\n",
        "review_generator = ....\n",
        "\n",
        "##########################\n",
        "## YOUR CODE HERE END   ##\n",
        "##########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhj8_NG_ms2T"
      },
      "source": [
        "pos_generated_reviews = review_generator(POS_TOKEN, max_length=50, num_return_sequences=3)\n",
        "pd.DataFrame(pos_generated_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxN9_D_Nm3jT"
      },
      "source": [
        "neg_generated_reviews = review_generator(NEG_TOKEN, max_length=50, num_return_sequences=3)\n",
        "pd.DataFrame(neg_generated_reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Congratulation\n",
        "You have explored different ways to generate text with a GPT-2-Transformer model"
      ],
      "metadata": {
        "id": "uSlYZeOVoX8F"
      }
    }
  ]
}